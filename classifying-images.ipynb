{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.11.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data\n",
    "\n",
    "The cat vs dog dataset we use is not a Keras package. It was posted on Kaggle.com as part of a Computer Vision problem in late 2013, when ConvNets were not yet so popular. \n",
    "\n",
    "The images are medium resolution JPGEs. It looks like this:\n",
    "\n",
    "![cats_vs_dogs_samples](https://s3.amazonaws.com/book.keras.io/img/ch5/cats_vs_dogs_samples.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's no surprise that the 2013 Kaggle cat vs dog competition was won by ConvNets. The best were able to achieve up to 95% accuracy. In our example we are still far from this accuracy, but during the Deep Learning course we have learned how to approach this value using different methods to improve the performance of neural networks. It should be noted that in this example we are training on approximately only 10% of the data that was used for the contest. \n",
    "After downloading the dataset and decompressing it, we are going to create a new dataset containing three subsets: a training set containing 1000 images of each class, a validation set with 500 images of each class, and finally a test set with 500 images of each class.\n",
    "\n",
    "Here we have a few lines of code that make us this distribution automatically:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path to the directory where the original\n",
    "# dataset was uncompressed\n",
    "original_dataset_dir = 'train'\n",
    "\n",
    "# The directory where we will\n",
    "# store our smaller dataset\n",
    "base_dir = 'cats_and_dogs_small'\n",
    "os.mkdir(base_dir)\n",
    "\n",
    "# Directories for our training,\n",
    "# validation and test splits\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "os.mkdir(train_dir)\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "os.mkdir(validation_dir)\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "os.mkdir(test_dir)\n",
    "\n",
    "# Directory with our training cat pictures\n",
    "train_cats_dir = os.path.join(train_dir, 'cats')\n",
    "os.mkdir(train_cats_dir)\n",
    "\n",
    "# Directory with our training dog pictures\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
    "os.mkdir(train_dogs_dir)\n",
    "\n",
    "# Directory with our validation cat pictures\n",
    "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
    "os.mkdir(validation_cats_dir)\n",
    "\n",
    "# Directory with our validation dog pictures\n",
    "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
    "os.mkdir(validation_dogs_dir)\n",
    "\n",
    "# Directory with our validation cat pictures\n",
    "test_cats_dir = os.path.join(test_dir, 'cats')\n",
    "os.mkdir(test_cats_dir)\n",
    "\n",
    "# Directory with our validation dog pictures\n",
    "test_dogs_dir = os.path.join(test_dir, 'dogs')\n",
    "os.mkdir(test_dogs_dir)\n",
    "\n",
    "# Copy first 1000 cat images to train_cats_dir\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join('train\\\\cats', fname)\n",
    "    dst = os.path.join(train_cats_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "# Copy next 500 cat images to validation_cats_dir\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1000, 1500)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join('validation\\\\cats', fname)\n",
    "    dst = os.path.join(validation_cats_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy next 500 cat images to test_cats_dir\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1500, 2000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join('test\\\\cats', fname)\n",
    "    dst = os.path.join(test_cats_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy first 1000 dog images to train_dogs_dir\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join('train\\\\dogs', fname)\n",
    "    dst = os.path.join(train_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy next 500 dog images to validation_dogs_dir\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1000, 1500)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join('validation\\\\dogs', fname)\n",
    "    dst = os.path.join(validation_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy next 500 dog images to test_dogs_dir\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1500, 2000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join('test\\\\dogs', fname)\n",
    "    dst = os.path.join(test_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, let's count how many pictures we have in each training split (train/validation/test):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training cat images: 1000\n"
     ]
    }
   ],
   "source": [
    "print('total training cat images:', len(os.listdir(train_cats_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training dog images: 1000\n"
     ]
    }
   ],
   "source": [
    "print('total training dog images:', len(os.listdir(train_dogs_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total validation cat images: 500\n"
     ]
    }
   ],
   "source": [
    "print('total validation cat images:', len(os.listdir(validation_cats_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total validation dog images: 500\n"
     ]
    }
   ],
   "source": [
    "print('total validation dog images:', len(os.listdir(validation_dogs_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total test cat images: 500\n"
     ]
    }
   ],
   "source": [
    "print('total test cat images:', len(os.listdir(test_cats_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total test dog images: 500\n"
     ]
    }
   ],
   "source": [
    "print('total test dog images:', len(os.listdir(test_dogs_dir)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So effectively we have 2000 training images, 1000 validation images and 1000 test images. In each of these subsets there are the same number of examples from each class: this is what is called a balanced binary classification system, which means that our classification accuracy will be an adequate metric of the success of our solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 148, 148, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 74, 74, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 72, 72, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 36, 36, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 34, 34, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 17, 17, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 15, 15, 128)       147584    \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 7, 7, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 6272)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               3211776   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,453,121\n",
      "Trainable params: 3,453,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(150,150,3)))\n",
    "model.add(layers.MaxPooling2D(2,2))\n",
    "model.add(layers.Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D(2,2))\n",
    "model.add(layers.Conv2D(128, (3,3), activation='relu'))  \n",
    "model.add(layers.MaxPooling2D(2,2))\n",
    "model.add(layers.Conv2D(128, (3,3), activation='relu'))  \n",
    "model.add(layers.MaxPooling2D(2,2))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the compilation step, we will be using `RMSprop`(lr=1e-4) as optimizer. As our network finished with a sigmoid, we are going to use binary cross entropy as loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "model.compile(keras.optimizers.RMSprop() , loss=keras.losses.binary_crossentropy, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'cats_and_dogs_small\\\\train',\n",
    "        target_size=(150, 150), \n",
    "        batch_size=20,\n",
    "        class_mode='binary') \n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        'cats_and_dogs_small\\\\test',\n",
    "        target_size=(150, 150), \n",
    "        batch_size=20,\n",
    "        class_mode='binary') \n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        'cats_and_dogs_small\\\\validation',\n",
    "        target_size=(150, 150),  \n",
    "        batch_size=20,\n",
    "        class_mode='binary') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at one of these generators: it takes us to a batch of 150x150 RGB images (dimensions `(20, 150, 150, 3)`) and binary tags (dimension `(20,)`). 20 is the number of examples in each batch (what we call the batch size). The generator generates these batches indefinitely: it runs a loop endlessly through all the images we have in the folder. That's why we have to type `break` to break the loop at some point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data batch shape: (20, 150, 150, 3)\n",
      "labels batch shape: (20,)\n"
     ]
    }
   ],
   "source": [
    "for data_batch, labels_batch in train_generator:\n",
    "    print('data batch shape:', data_batch.shape)\n",
    "    print('labels batch shape:', labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make the fit. In this case, as what we have is a generator, we use fit_generator. We are going to run 30 epochs and use the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nacho\\AppData\\Local\\Temp\\ipykernel_8324\\3774117403.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = model.fit_generator(generator = train_generator, steps_per_epoch = 20, epochs=30)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "20/20 [==============================] - 40s 2s/step - loss: 0.8691 - acc: 0.4675\n",
      "Epoch 2/30\n",
      "20/20 [==============================] - 42s 2s/step - loss: 0.6947 - acc: 0.5050\n",
      "Epoch 3/30\n",
      "20/20 [==============================] - 40s 2s/step - loss: 0.7201 - acc: 0.5700\n",
      "Epoch 4/30\n",
      "20/20 [==============================] - 40s 2s/step - loss: 0.6939 - acc: 0.5375\n",
      "Epoch 5/30\n",
      "20/20 [==============================] - 35s 2s/step - loss: 0.6901 - acc: 0.5825\n",
      "Epoch 6/30\n",
      "20/20 [==============================] - 41s 2s/step - loss: 0.6903 - acc: 0.5900\n",
      "Epoch 7/30\n",
      "20/20 [==============================] - 43s 2s/step - loss: 0.6707 - acc: 0.6400\n",
      "Epoch 8/30\n",
      "20/20 [==============================] - 46s 2s/step - loss: 0.7383 - acc: 0.6225\n",
      "Epoch 9/30\n",
      "20/20 [==============================] - 39s 2s/step - loss: 0.6528 - acc: 0.6050\n",
      "Epoch 10/30\n",
      "20/20 [==============================] - 40s 2s/step - loss: 0.6319 - acc: 0.6375\n",
      "Epoch 11/30\n",
      "20/20 [==============================] - 42s 2s/step - loss: 0.6365 - acc: 0.6525\n",
      "Epoch 12/30\n",
      "20/20 [==============================] - 43s 2s/step - loss: 0.6081 - acc: 0.6900\n",
      "Epoch 13/30\n",
      "20/20 [==============================] - 37s 2s/step - loss: 0.6067 - acc: 0.6825\n",
      "Epoch 14/30\n",
      "20/20 [==============================] - 35s 2s/step - loss: 0.5965 - acc: 0.7125\n",
      "Epoch 15/30\n",
      "20/20 [==============================] - 34s 2s/step - loss: 0.5499 - acc: 0.7450\n",
      "Epoch 16/30\n",
      "20/20 [==============================] - 34s 2s/step - loss: 0.5904 - acc: 0.6775\n",
      "Epoch 17/30\n",
      "20/20 [==============================] - 38s 2s/step - loss: 0.5474 - acc: 0.7250\n",
      "Epoch 18/30\n",
      "20/20 [==============================] - 43s 2s/step - loss: 0.5805 - acc: 0.7325\n",
      "Epoch 19/30\n",
      "20/20 [==============================] - 40s 2s/step - loss: 0.5222 - acc: 0.7525\n",
      "Epoch 20/30\n",
      "20/20 [==============================] - 37s 2s/step - loss: 0.5490 - acc: 0.7325\n",
      "Epoch 21/30\n",
      "20/20 [==============================] - 39s 2s/step - loss: 0.5228 - acc: 0.7400\n",
      "Epoch 22/30\n",
      "20/20 [==============================] - 47s 2s/step - loss: 0.5290 - acc: 0.7425\n",
      "Epoch 23/30\n",
      "20/20 [==============================] - 35s 2s/step - loss: 0.5757 - acc: 0.7275\n",
      "Epoch 24/30\n",
      "20/20 [==============================] - 34s 2s/step - loss: 0.4885 - acc: 0.7550\n",
      "Epoch 25/30\n",
      "20/20 [==============================] - 35s 2s/step - loss: 0.4821 - acc: 0.7825\n",
      "Epoch 26/30\n",
      "20/20 [==============================] - 34s 2s/step - loss: 0.4566 - acc: 0.8100\n",
      "Epoch 27/30\n",
      "20/20 [==============================] - 34s 2s/step - loss: 0.5043 - acc: 0.7450\n",
      "Epoch 28/30\n",
      "20/20 [==============================] - 33s 2s/step - loss: 0.4434 - acc: 0.7900\n",
      "Epoch 29/30\n",
      "20/20 [==============================] - 34s 2s/step - loss: 0.4961 - acc: 0.7525\n",
      "Epoch 30/30\n",
      "20/20 [==============================] - 34s 2s/step - loss: 0.4468 - acc: 0.8000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(generator = train_generator, steps_per_epoch = 20, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 31s 610ms/step - loss: 0.6315 - acc: 0.7140\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Saving the model after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('modelo_perrosGatos.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now load the saved model and evaluate it using the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 32s 636ms/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "modelo = tf.keras.models.load_model('modelo_perrosGatos.h5')\n",
    "predicciones = modelo.predict(validation_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to flip our images and retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nacho\\AppData\\Local\\Temp\\ipykernel_8324\\4167916078.py:27: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = model.fit_generator(generator = train_generator, steps_per_epoch = 20, epochs=30)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "20/20 [==============================] - 61s 3s/step - loss: 0.5281 - acc: 0.7422\n",
      "Epoch 2/30\n",
      "20/20 [==============================] - 58s 3s/step - loss: 0.4986 - acc: 0.7564\n",
      "Epoch 3/30\n",
      "20/20 [==============================] - 54s 3s/step - loss: 0.5179 - acc: 0.7676\n",
      "Epoch 4/30\n",
      "20/20 [==============================] - 55s 3s/step - loss: 0.5025 - acc: 0.7750\n",
      "Epoch 5/30\n",
      "20/20 [==============================] - 55s 3s/step - loss: 0.5184 - acc: 0.7266\n",
      "Epoch 6/30\n",
      "20/20 [==============================] - 55s 3s/step - loss: 0.4905 - acc: 0.7812\n",
      "Epoch 7/30\n",
      "20/20 [==============================] - 54s 3s/step - loss: 0.5053 - acc: 0.7469\n",
      "Epoch 8/30\n",
      "20/20 [==============================] - 59s 3s/step - loss: 0.4808 - acc: 0.7922\n",
      "Epoch 9/30\n",
      "20/20 [==============================] - 57s 3s/step - loss: 0.4841 - acc: 0.7656\n",
      "Epoch 10/30\n",
      "20/20 [==============================] - 54s 3s/step - loss: 0.4367 - acc: 0.7953\n",
      "Epoch 11/30\n",
      "20/20 [==============================] - 55s 3s/step - loss: 0.4334 - acc: 0.7906\n",
      "Epoch 12/30\n",
      "20/20 [==============================] - 57s 3s/step - loss: 0.4557 - acc: 0.7922\n",
      "Epoch 13/30\n",
      "20/20 [==============================] - 55s 3s/step - loss: 0.4173 - acc: 0.8062\n",
      "Epoch 14/30\n",
      "20/20 [==============================] - 55s 3s/step - loss: 0.4327 - acc: 0.7844\n",
      "Epoch 15/30\n",
      "20/20 [==============================] - 55s 3s/step - loss: 0.4404 - acc: 0.7922\n",
      "Epoch 16/30\n",
      "20/20 [==============================] - 56s 3s/step - loss: 0.4162 - acc: 0.8203\n",
      "Epoch 17/30\n",
      "20/20 [==============================] - 54s 3s/step - loss: 0.4173 - acc: 0.8172\n",
      "Epoch 18/30\n",
      "20/20 [==============================] - 62s 3s/step - loss: 0.4034 - acc: 0.8141\n",
      "Epoch 19/30\n",
      "20/20 [==============================] - 54s 3s/step - loss: 0.3889 - acc: 0.8157\n",
      "Epoch 20/30\n",
      "20/20 [==============================] - 56s 3s/step - loss: 0.3968 - acc: 0.8344\n",
      "Epoch 21/30\n",
      "20/20 [==============================] - 55s 3s/step - loss: 0.4054 - acc: 0.8109\n",
      "Epoch 22/30\n",
      "20/20 [==============================] - 56s 3s/step - loss: 0.3777 - acc: 0.8422\n",
      "Epoch 23/30\n",
      "20/20 [==============================] - 56s 3s/step - loss: 0.3910 - acc: 0.8438\n",
      "Epoch 24/30\n",
      "20/20 [==============================] - 54s 3s/step - loss: 0.3438 - acc: 0.8542\n",
      "Epoch 25/30\n",
      "20/20 [==============================] - 54s 3s/step - loss: 0.3738 - acc: 0.8157\n",
      "Epoch 26/30\n",
      "20/20 [==============================] - 62s 3s/step - loss: 0.3915 - acc: 0.8125\n",
      "Epoch 27/30\n",
      "20/20 [==============================] - 55s 3s/step - loss: 0.3733 - acc: 0.8446\n",
      "Epoch 28/30\n",
      "20/20 [==============================] - 54s 3s/step - loss: 0.4354 - acc: 0.8125\n",
      "Epoch 29/30\n",
      "20/20 [==============================] - 54s 3s/step - loss: 0.3661 - acc: 0.8328\n",
      "Epoch 30/30\n",
      "20/20 [==============================] - 55s 3s/step - loss: 0.4230 - acc: 0.8266\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2, \n",
    "        horizontal_flip=True)\n",
    "\n",
    "\n",
    "# Carga las imágenes de entrenamiento desde un directorio\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'cats_and_dogs_small\\\\train',\n",
    "        target_size=(150, 150),  # Redimensiona las imágenes a 150x150 píxeles\n",
    "        batch_size=32,\n",
    "        class_mode='binary') \n",
    "test_generator = train_datagen.flow_from_directory(\n",
    "        'cats_and_dogs_small\\\\test',\n",
    "        target_size=(150, 150),  # Redimensiona las imágenes a 150x150 píxeles\n",
    "        batch_size=32,\n",
    "        class_mode='binary') \n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "        'cats_and_dogs_small\\\\validation',\n",
    "        target_size=(150, 150),  # Redimensiona las imágenes a 150x150 píxeles\n",
    "        batch_size=32,\n",
    "        class_mode='binary') \n",
    "\n",
    "history = model.fit_generator(generator = train_generator, steps_per_epoch = 20, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 33s 1s/step - loss: 0.5340 - acc: 0.7480\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FLiping the images, our accuracy changes from 0.714 to 0.748"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
